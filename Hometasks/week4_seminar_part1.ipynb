{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Estimators\n",
    "\n",
    "Сегодня научимся рабоатть с:\n",
    "1. tf.estimators\n",
    "2. tf.data\n",
    "3. tensorboard\n",
    "4. tf.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nullkatar/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.keras.datasets import imdb\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager Execution\n",
    "[TensorFlow's eager execution](https://www.tensorflow.org/guide/eager) is an imperative programming environment that evaluates operations immediately, without building graphs.\n",
    "\n",
    "<img src=\"http://www.netlore.ru/upload/files/19/large_p19hom1f751nk1c40ml57hu2skj.jpg\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как проверять размерности?\n",
    "\n",
    "При прототипировании новых архитектур, нужно следить за размерностями тензоров. Как это делать в tensorflow? \n",
    "\n",
    "**numpy-like** экспериментирование:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y shape: (64,)\n",
      "X shape: (64, 20, 100)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "time_steps = 20\n",
    "emb_size = 100\n",
    "\n",
    "# создаем игрушечный датасет\n",
    "x_toy = np.random.rand(batch_size, time_steps, emb_size)\n",
    "y_toy = np.random.randint(0, 9, batch_size)\n",
    "\n",
    "print('Y shape: {}'.format(y_toy.shape))\n",
    "print('X shape: {}'.format(x_toy.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/73d826d4c2363701b88e3e234fe3b8756c0f9671/3-Figure1-1.png\" width=700>\n",
    "\n",
    "Реализуем операцию свертки над нашими данными. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# переводим переменные в тензоры\n",
    "\n",
    "x_toy = tf.convert_to_tensor(x_toy, dtype=tf.float32)\n",
    "y_toy = tf.convert_to_tensor(y_toy, dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# случайно дропаем 10% слов (зануляем)\n",
    "\n",
    "x_toy = tf.layers.dropout(x_toy,\n",
    "                          rate=0.1,\n",
    "                          noise_shape=[batch_size, time_steps, 1],\n",
    "                          training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.60919577 1.0656664  0.31323364 ... 0.38649768 0.85007447 0.5085799 ]\n",
      "  [0.40480652 0.5662459  0.7426091  ... 0.32881692 0.69997364 0.7571073 ]\n",
      "  ...\n",
      "  [0.7822716  0.07132865 0.02138079 ... 0.50230515 0.5138934  1.0532569 ]\n",
      "  [0.3639544  0.40157005 0.9155454  ... 0.09712479 1.089555   1.0157101 ]\n",
      "  [0.67328674 1.0308605  0.0679797  ... 0.78731126 0.8427862  0.91051745]]\n",
      "\n",
      " [[0.6855246  1.0526739  1.0584143  ... 0.05824028 0.08301507 0.2710551 ]\n",
      "  [0.58707875 0.89950144 0.7581761  ... 0.15624158 0.06752342 0.70731944]\n",
      "  [0.8280965  0.7006203  0.61123955 ... 0.06830905 0.02702304 0.38327578]\n",
      "  ...\n",
      "  [0.5886233  0.20313919 0.17277178 ... 0.5318525  0.536158   0.4839056 ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.397312   0.6482189  0.7136935  ... 0.635741   0.13885258 0.18561363]]\n",
      "\n",
      " [[0.81494445 0.11731581 0.2807727  ... 0.2318563  0.9170693  0.10700139]\n",
      "  [1.011232   0.32624942 0.03388381 ... 0.36635447 0.6249047  0.1998254 ]\n",
      "  [0.95799565 1.0699906  0.8796428  ... 0.774725   0.7739172  0.04772796]\n",
      "  ...\n",
      "  [0.72707033 1.0243069  0.6214982  ... 0.42553747 0.7357369  1.031204  ]\n",
      "  [1.0806535  0.11677632 0.4664655  ... 0.11743084 0.7779121  0.69575924]\n",
      "  [0.15463585 0.632326   0.1634691  ... 0.25028986 0.73525584 0.9237942 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.985987   1.0836946  0.6067486  ... 0.3499823  0.55313706 0.9425802 ]\n",
      "  [0.27057752 0.86631596 0.29939404 ... 0.8493362  0.9554112  0.50883776]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.32896236 0.9911131  0.9334072  ... 0.1070435  0.4410842  0.2665042 ]\n",
      "  [0.30979708 1.0296557  0.14766166 ... 0.78074855 0.349593   0.18593399]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.43575525 0.6855878  0.28521723 ... 0.52371734 0.5961502  0.9662492 ]\n",
      "  [0.33730906 0.3044999  0.2022114  ... 0.69128084 0.89770424 0.03039153]\n",
      "  [0.7839613  0.97124267 0.34769884 ... 1.0461887  0.43216088 0.4370777 ]\n",
      "  ...\n",
      "  [0.5722554  0.5072612  0.48934534 ... 0.9576209  0.43134806 0.1745771 ]\n",
      "  [0.6976916  0.3320849  0.6543704  ... 0.87869626 0.4715665  0.79945606]\n",
      "  [0.7760056  0.43229017 0.02520827 ... 0.14785346 0.8321557  0.19377546]]\n",
      "\n",
      " [[0.4628729  0.43219793 0.45151857 ... 0.9521888  0.98258746 0.11556555]\n",
      "  [0.5091848  0.7909302  0.53091294 ... 0.4192956  1.0010946  0.9164946 ]\n",
      "  [0.69455314 0.75463855 0.59496856 ... 0.69614667 0.52613235 0.7560664 ]\n",
      "  ...\n",
      "  [1.0026001  0.92861253 0.5527115  ... 0.24001247 0.7848493  0.15289566]\n",
      "  [0.26616514 0.31017634 0.71423256 ... 0.91299397 0.08723385 0.78474146]\n",
      "  [0.26012462 0.8498835  0.2840865  ... 0.50551814 0.2776212  0.521391  ]]], shape=(64, 20, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# можем посмотреть на сам тензор\n",
    "\n",
    "print(x_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 9, 8)\n",
      "(64, 8)\n",
      "(64, 2)\n"
     ]
    }
   ],
   "source": [
    "# применяем свертку\n",
    "\n",
    "conv_2 = tf.layers.conv1d(x_toy,\n",
    "                          filters=8,\n",
    "                          kernel_size=3,\n",
    "                          strides=2)\n",
    "\n",
    "print(conv_2.shape)\n",
    "\n",
    "# макспулинг\n",
    "max_pool = tf.reduce_max(conv_2, axis=1)\n",
    "print(max_pool.shape)\n",
    "\n",
    "# денс-слой\n",
    "fc = tf.layers.dense(max_pool, 2)\n",
    "print(fc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "# softmax на выходе\n",
    "\n",
    "fc_soft = tf.nn.softmax(fc)\n",
    "\n",
    "# предсказания\n",
    "preds = tf.argmax(fc_soft, axis=1)\n",
    "\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных\n",
    "\n",
    "Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 6s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 200)\n",
      "x_test shape: (25000, 200)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "sentence_size = 200\n",
    "model_dir = 'model_dir'\n",
    "\n",
    "pad_id = 0\n",
    "start_id = 1\n",
    "oov_id = 2\n",
    "index_offset = 2\n",
    "\n",
    "print(\"Loading data...\")\n",
    "(x_train_variable, y_train), (x_test_variable, y_test) = imdb.load_data(num_words=vocab_size,\n",
    "                                                                        start_char=start_id,\n",
    "                                                                        oov_char=oov_id,\n",
    "                                                                        index_from=index_offset)\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "print(len(y_train), \"train sequences\")\n",
    "print(len(y_test), \"test sequences\")\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "x_train = sequence.pad_sequences(x_train_variable, \n",
    "                                 maxlen=sentence_size,\n",
    "                                 truncating='post',\n",
    "                                 padding='post',\n",
    "                                 value=pad_id)\n",
    "\n",
    "x_test = sequence.pad_sequences(x_test_variable, \n",
    "                                maxlen=sentence_size,\n",
    "                                truncating='post',\n",
    "                                padding='post', \n",
    "                                value=pad_id)\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.data\n",
    "\n",
    "[The tf.data API](https://www.tensorflow.org/api_docs/python/tf/data) enables you to build complex input pipelines from simple, reusable pieces. The tf.data API makes it easy to deal with large amounts of data, different data formats, and complicated transformations.\n",
    "\n",
    "\n",
    "he tf.data module contains a collection of classes that allows you to easily load data, manipulate it, and pipe it into your model. [This document](https://www.tensorflow.org/guide/datasets_for_estimators) introduces the API by walking through two simple examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 256,\n",
    "    'num_epochs': 5,\n",
    "    'train_size': int(len(x_train) * 0.9)\n",
    "}\n",
    "\n",
    "def input_fn(data, labels, params, is_training):\n",
    "    # tf.data.TextLineDataset\n",
    "    # tf.data.TFRecordDataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "\n",
    "    if is_training:\n",
    "        # перемешиваем данные для каждой эпохи\n",
    "        dataset = dataset.shuffle(buffer_size=params['train_size'])\n",
    "        dataset = dataset.repeat(count=params['num_epochs'])\n",
    "\n",
    "    dataset = dataset.batch(params['batch_size'])\n",
    "    dataset = dataset.map(lambda x, y: ({'data': x}, y))\n",
    "    # можно попросить tensorflow заранее считать батчи, чтобы GPU не простаивала\n",
    "    dataset = dataset.prefetch(buffer_size=100)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to debug?\n",
    "\n",
    "Как посмотреть на то, что выдает tf.data.Dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RESTART THE KERNEL\n",
    "# DISABLE EAGER EXECUTION\n",
    "\n",
    "dataset = input_fn(x_train, y_train, params=params, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterator = dataset.make_initializable_iterator()\n",
    "batch = iterator.get_next()\n",
    "init = iterator.initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for _ in range(5):\n",
    "        x, y = sess.run(batch)\n",
    "        print(x['data'].shape)\n",
    "        print(y.shape)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premade estimators\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/tensorflow_programming_environment.png\" width=600>\n",
    "\n",
    "**An Estimator** is TensorFlow's high-level representation of a complete model. It handles the details of initialization, logging, saving and restoring, and many other features so you can concentrate on your model.\n",
    "\n",
    "Estimators encapsulate the following actions:\n",
    "\n",
    "* training\n",
    "* evaluation\n",
    "* prediction\n",
    "* export for serving\n",
    "\n",
    "\n",
    "Four steps to become an estimator master:\n",
    "\n",
    "1. ~~Write one or more dataset importing functions~~\n",
    "2. Define the [feature columns](https://www.tensorflow.org/guide/feature_columns)\n",
    "3. Instantiate the relevant pre-made Estimator\n",
    "4. Call a training, evaluation, or inference method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "В качестве входа для логистической регрессии используем Bag-of-Words\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*j3HUg18QwjDJTJwW9ja5-Q.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_classifiers = {}\n",
    "\n",
    "# Определяем функцию, которая будет запускать обучение и валидацию\n",
    "def train_and_evaluate(classifier):\n",
    "    all_classifiers[classifier.model_dir] = classifier\n",
    "    classifier.train(lambda: input_fn(x_train, y_train, params=params, is_training=True))\n",
    "    results = classifier.evaluate(lambda: input_fn(x_test, y_test, params=params, is_training=False))\n",
    "\n",
    "    print()\n",
    "    for key, value in results.items():\n",
    "        print(f'{key}: {value}')\n",
    "    \n",
    "    # ресетим граф\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow = tf.feature_column.categorical_column_with_identity(key='data',\n",
    "                                                         num_buckets=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.estimator.RunConfig(tf_random_seed=123,\n",
    "                                model_dir=os.path.join(model_dir, 'bow_sparse'),\n",
    "                                save_summary_steps=5)\n",
    "\n",
    "classifier = tf.estimator.LinearClassifier(feature_columns=[bow],\n",
    "                                           config=config,\n",
    "                                           optimizer='Adam',\n",
    "                                           n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "Из sparse представления делаем dense\n",
    "\n",
    "Меняем вход **(bs, vocab_size) -> (bs, time_steps, embedding_size)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 50\n",
    "\n",
    "# (bs, time_steps, embedding_size) -> (bs, embedding_size)\n",
    "word_embedding_column = tf.feature_column.embedding_column(categorical_column=bow,\n",
    "                                                           dimension=embedding_size,\n",
    "                                                           combiner='mean',\n",
    "                                                           initializer=tf.truncated_normal_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['num_epochs'] = 10\n",
    "\n",
    "config = tf.estimator.RunConfig(tf_random_seed=123,\n",
    "                                model_dir=os.path.join(model_dir, 'embeddings'),\n",
    "                                save_summary_steps=5)\n",
    "\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    hidden_units=[32, 16],\n",
    "    activation_fn=tf.nn.tanh,\n",
    "    feature_columns=[word_embedding_column],\n",
    "    n_classes=2,\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Custom Estimators\n",
    "\n",
    "<img src=\"https://sun1-2.userapi.com/c831409/v831409088/1596d6/3ZNzHyVKY_w.jpg\" width=350>\n",
    "\n",
    "Своя архитектура, свои метрики, свои оптимайзеры и так далее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# определяем архитектуру модели\n",
    "\n",
    "def build_model(features, params, is_training):\n",
    "    with tf.name_scope('embeddings'):\n",
    "        emb_matrix = tf.get_variable('embedding_matrix',\n",
    "                                     shape=[vocab_size, sentence_size],\n",
    "                                     dtype=tf.float32)\n",
    "\n",
    "        # (batch_size, time_steps, emb_dim)\n",
    "        embeddings = tf.nn.embedding_lookup(emb_matrix, features['data'])\n",
    "        # (batch_size, emb_dim)\n",
    "        mean_embs = tf.reduce_mean(embeddings, axis=1)\n",
    "    \n",
    "    with tf.name_scope('fc_1'):\n",
    "        out = tf.layers.dense(mean_embs, 50)\n",
    "        # out = tf.layers.batch_normalization(out, training=is_training)\n",
    "        out = tf.nn.tanh(out)\n",
    "\n",
    "    with tf.name_scope('fc_2'):\n",
    "        out = tf.layers.dense(out, 32)\n",
    "        out = tf.nn.tanh(out)\n",
    "        \n",
    "    with tf.name_scope('fc_3'):\n",
    "        out = tf.layers.dense(out, 2)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# определяем Estimator. Говорим, какая у нас функция потерь, метрики и оптимайзер\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    \n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    with tf.variable_scope('model'):\n",
    "        logits = build_model(features, params, is_training)\n",
    "        \n",
    "    preds = tf.argmax(logits, axis=1)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {'preds': preds}\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=predictions)\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(preds, labels), tf.float32))\n",
    "    labels = tf.one_hot(labels, 2)\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        with tf.variable_scope('metrics'):\n",
    "            eval_metrics = {'accuracy': tf.metrics.mean(accuracy)}\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metrics)\n",
    "    \n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    \n",
    "    global_step = tf.train.get_global_step()\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# конфиг для модели\n",
    "\n",
    "config = tf.estimator.RunConfig(tf_random_seed=123,\n",
    "                               model_dir=os.path.join(model_dir, 'custom'),\n",
    "                               save_summary_steps=5)\n",
    "\n",
    "# Estimator object\n",
    "estimator = tf.estimator.Estimator(model_fn,\n",
    "                                   params=params,\n",
    "                                   config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# запускаем обучение\n",
    "\n",
    "train_and_evaluate(estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Как предсказывать?\n",
    "\n",
    "В **tf.estimator.EstimatorSpec** можно передавать любые тензоры, которые будут доступны при предсказании."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = estimator.predict(lambda: input_fn(x_test, y_test, params=params, is_training=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for p in predictions:\n",
    "    preds.append(p['preds'])\n",
    "\n",
    "preds = np.array(preds, int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard: Visualizing Learning\n",
    "\n",
    "Чтобы посмотреть на графики изменения лосса и метрик, в терминале:\n",
    "\n",
    "> tensorboard --logdir **model_dir_path**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
